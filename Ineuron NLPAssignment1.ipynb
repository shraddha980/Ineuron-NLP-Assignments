{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1de71ad",
   "metadata": {},
   "source": [
    "1.\tExplain One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f00c74c",
   "metadata": {},
   "source": [
    "One hot encoding is a technique that we use to represent categorical variables as numerical values in a machine learning model.\n",
    "\n",
    "It allows the use of categorical variables in models that require numerical input.\n",
    "It can improve model performance by providing more information to the model about the categorical variable.\n",
    "It can help to avoid the problem of ordinality, which can occur when a categorical variable has a natural ordering (e.g. “small”, “medium”, “large”).\n",
    "The disadvantages of using one hot encoding include:\n",
    "It can lead to increased dimensionality, as a separate column is created for each category in the variable. This can make the model more complex and slow to train.\n",
    " \n",
    "It can lead to sparse data, as most observations will have a value of 0 in most of the one-hot encoded columns.\n",
    " \n",
    "It can lead to overfitting, especially if there are many categories in the variable and the sample size is relatively small.\n",
    " \n",
    " One-hot-encoding is a powerful technique to treat categorical data, but it can lead to increased dimensionality, sparsity, and overfitting. It is important to use it cautiously and consider other methods such as ordinal encoding or binary encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb6c268",
   "metadata": {},
   "source": [
    "2.\tExplain Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4415328",
   "metadata": {},
   "source": [
    "The bag-of-words (BOW) model is a representation that turns arbitrary text into fixed-length vectors by counting how many times each word appears. This process is often referred to as vectorization.\n",
    "\n",
    "Step 1: Determine the Vocabulary\n",
    "We first define our vocabulary, which is the set of all words found in our document set. The only words that are found in the 3 documents above are: the, cat, sat, in, the, hat, and with.\n",
    "\n",
    "Step 2: Count\n",
    "To vectorize our documents, all we have to do is count how many times each word appears:\n",
    "\n",
    "Now we have length-6 vectors for each document!\n",
    "\n",
    "the cat sat: [1, 1, 1, 0, 0, 0]\n",
    "the cat sat in the hat: [2, 1, 1, 1, 1, 0]\n",
    "the cat with the hat: [2, 1, 0, 0, 1, 1]\n",
    "Notice that we lose contextual information, e.g. where in the document the word appeared, when we use BOW. It’s like a literal bag-of-words: it only tells you what words occur in the document, not where they occurred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4349eec",
   "metadata": {},
   "source": [
    "3.  Explain Bag of N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386cc199",
   "metadata": {},
   "source": [
    "The Bag of N-Grams is a fundamental concept in NLP, leveraging the simplicity of tokenization and the richness of context analysis. In essence, it involves breaking down a text into its constituent n-grams (sequences of 'n' consecutive words) and creating a bag, or set, of these n-grams.\n",
    "\n",
    "    N-grams play an important role in natural language processing (NLP) and text analysis. They capture local patterns, aiding in text representation and structure understanding. N-grams are essential for feature extraction in machine learning models, providing a straightforward method to convert text into numerical features. Additionally, they form the foundation for probabilistic language models, measure document similarity, assist in spell checking, enhance speech recognition, optimize search engine results, facilitate text generation, and contribute to named entity recognition and information extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8284b32",
   "metadata": {},
   "source": [
    "4.\tExplain TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e47ae",
   "metadata": {},
   "source": [
    "Term Frequency - Inverse Document Frequency (TF-IDF) is a widely used statistical method in natural language processing and information retrieval. It measures how important a term is within a document relative to a collection of documents (i.e., relative to a corpus)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128fac04",
   "metadata": {},
   "source": [
    "5.\tWhat is OOV problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fa41bf",
   "metadata": {},
   "source": [
    "Out-of-vocabulary (OOV) are terms that are not part of the normal lexicon found in a natural language processing environment. \n",
    "\n",
    "In speech recognition, it’s the audio signal that contains these terms. Word vectors are the mathematical equivalent of word meaning. But the limitation of word embeddings is that the words need to have been seen before in the training data.\n",
    "\n",
    "When a word that’s not in the training set occurs in real data, this causes a problem. There are various techniques to avoid a zero-probability occurrence including smoothing and replacing the word a synonym."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97413b0",
   "metadata": {},
   "source": [
    "6.\tWhat are word embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9da908b",
   "metadata": {},
   "source": [
    "Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bb4cf5",
   "metadata": {},
   "source": [
    "7.\tExplain Continuous bag of words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cfa5c7",
   "metadata": {},
   "source": [
    "The Continuous Bag of Words (CBOW) model is a popular method for training word embeddings, which are representations of words in a numerical vector space. The goal of CBOW is to predict a target word given the context words in a sentence. This is in contrast to the skip-gram model, which predicts context words given a target word.\n",
    "\n",
    "The architecture of the CBOW model is relatively simple, consisting of an input layer, a hidden layer, and an output layer. The input layer is used to represent the context words, the hidden layer is used to learn the word embeddings, and the output layer is used to predict the target word.\n",
    "\n",
    "The input layer is typically represented by a one-hot encoded vector, where each element in the vector corresponds to a specific word in the vocabulary. For example, if the vocabulary contains 10,000 words, the input layer will have 10,000 elements.\n",
    "\n",
    "The hidden layer is where the word embeddings are learned. It is a dense layer, with each neuron representing a specific word in the vocabulary. The number of neurons in the hidden layer is the same as the number of words in the vocabulary.\n",
    "\n",
    "The output layer is also a dense layer, with each neuron representing a specific word in the vocabulary. The number of neurons in the output layer is the same as the number of words in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522c4751",
   "metadata": {},
   "source": [
    "8.\tExplain SkipGram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f725dc63",
   "metadata": {},
   "source": [
    "Skip-gram is a popular algorithm used in natural language processing (NLP), specifically in word embedding techniques. It is a method for learning word representations in a vector space, often used in the context of word2vec models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae0c15f",
   "metadata": {},
   "source": [
    "9.\tExplain Glove Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6910f1b",
   "metadata": {},
   "source": [
    "Global Vectors for Word Representation, or GloVe, is an “unsupervised learning algorithm for obtaining vector representations for words.” Simply put, GloVe allows us to take a corpus of text, and intuitively transform each word in that corpus into a position in a high-dimensional space. This means that similar words will be placed together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994651d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
