{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c81cc958",
   "metadata": {},
   "source": [
    "1.\tWhat are Sequence-to-sequence models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b34dd",
   "metadata": {},
   "source": [
    "Sequence to Sequence (often abbreviated to seq2seq) models is a special class of Recurrent Neural Network architectures that we typically use (but not restricted) to solve complex Language problems like Machine Translation, Question Answering, creating Chatbots, Text Summarization, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8f5745",
   "metadata": {},
   "source": [
    "2.\tWhat are the Problem with Vanilla RNNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b23e13",
   "metadata": {},
   "source": [
    "Due to the vanishing gradient problem, vanilla RNNs tend to have a “short-term memory”, i.e., they can struggle to maintain the influence of information from earlier time steps as the sequence gets longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aa4920",
   "metadata": {},
   "source": [
    "3.\tWhat is Gradient clipping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e295ff3e",
   "metadata": {},
   "source": [
    "Gradient clipping is a technique used during the training of deep neural networks to prevent the exploding gradient problem. This problem occurs when the gradients of the network's loss with respect to the weights become excessively large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6528b7c3",
   "metadata": {},
   "source": [
    "4.\tExplain Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b4b53d",
   "metadata": {},
   "source": [
    "The attention mechanism is a technique used in machine learning and natural language processing to increase model accuracy by focusing on relevant data. It enables the model to focus on certain areas of the input data, giving more weight to crucial features and disregarding unimportant ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab6bab5",
   "metadata": {},
   "source": [
    "5.\tExplain Conditional random fields (CRFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e00d043",
   "metadata": {},
   "source": [
    "Conditional random fields are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \"neighbouring\" samples, a CRF can take context into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a2488",
   "metadata": {},
   "source": [
    "6.\tExplain self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e18f0b9",
   "metadata": {},
   "source": [
    "Self-attention is a mechanism used in machine learning, particularly in natural language processing (NLP) and computer vision tasks, to capture dependencies and relationships within input sequences. It allows the model to identify and weigh the importance of different parts of the input sequence by attending to itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d674500",
   "metadata": {},
   "source": [
    "7.\tWhat is Bahdanau Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff53fba",
   "metadata": {},
   "source": [
    "The Bahdanau Attention Mechanism, also known as Additive Attention, is like a spotlight that helps a machine learning model focus on the most relevant parts of a long piece of information when making decisions, just like how you pay attention to different words when reading a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c709329",
   "metadata": {},
   "source": [
    "8.\tWhat is a Language Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b56784",
   "metadata": {},
   "source": [
    "Introduction. A language model in NLP is a probabilistic statistical model that determines the probability of a given sequence of words occurring in a sentence based on the previous words. It helps to predict which word is more likely to appear next in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62586ccf",
   "metadata": {},
   "source": [
    "9.\tWhat is Multi-Head Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca0251c",
   "metadata": {},
   "source": [
    "Introduced by Vaswani et al. in Attention Is All You Need. Multi-head Attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76542da6",
   "metadata": {},
   "source": [
    "10.\tWhat is Bilingual Evaluation Understudy (BLEU)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0ee8dde",
   "metadata": {},
   "source": [
    "BLEU (BiLingual Evaluation Understudy) is a metric for automatically evaluating machine-translated text. The BLEU score is a number between zero and one that measures the similarity of the machine-translated text to a set of high quality reference translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba8de25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
