{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77254614",
   "metadata": {},
   "source": [
    "1.\tExplain ROUGE metric evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c07bb3",
   "metadata": {},
   "source": [
    "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It is essentially a set of metrics for evaluating automatic summarization of texts as well as machine translations. It works by comparing an automatically produced summary or translation against a set of reference summaries (typically human-produced)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5655f6",
   "metadata": {},
   "source": [
    "2.\tExplain Masked Language Modeling (MLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f27da",
   "metadata": {},
   "source": [
    "MLM is a language model trained to predict the missing words in a sentence based on the context provided by the surrounding words. This is done by masking some of the words in the input text and training the model to predict the masked words based on the context of the non-masked words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe4384c",
   "metadata": {},
   "source": [
    "3.\tExplain Next Sentence Prediction (NSP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb6e611",
   "metadata": {},
   "source": [
    "NSP is like a fitness trainer for language models. It predicts the next sentence in chunks of text which helps them understand meaning, sentence structure, and rules of language. Think about NSP as a key that opens up a wide range of contexts and helps beat data limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183e93c1",
   "metadata": {},
   "source": [
    "4.\tWhat is Matthews evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ff09d",
   "metadata": {},
   "source": [
    "Matthew's correlation coefficient, also abbreviated as MCC was invented by Brian Matthews in 1975. MCC is a statistical tool used for model evaluation. Its job is to gauge or measure the difference between the predicted values and actual values and is equivalent to chi-square statistics for a 2 x 2 contingency table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02302a1d",
   "metadata": {},
   "source": [
    "5.\tWhat is Matthews Correlation Coefficient (MCC)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ee73d",
   "metadata": {},
   "source": [
    "Matthews Correlation Coefficient The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary (two-class) classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between −1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and −1 indicates total disagreement between prediction and observation. The statistic is also known as the phi coefficient. MCC is related to the chi-square statistic for a 2×2 contingency table\n",
    "\n",
    "|MCC|=χ2n−−−√\n",
    "where n is the total number of observations.\n",
    "\n",
    "While there is no perfect way of describing the confusion matrix of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the best such measures. Other measures, such as the proportion of correct predictions (also termed accuracy), are not useful when the two classes are of very different sizes. For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification.\n",
    "\n",
    "The MCC can be calculated directly from the confusion matrix using the formula:\n",
    "\n",
    "= { } In this equation, TP is the number of true positives, TN the number of true negatives, FP the number of false positives and FN the number of false negatives. If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98853ee",
   "metadata": {},
   "source": [
    "6.\tExplain Semantic Role Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e920b6",
   "metadata": {},
   "source": [
    "In natural language processing, semantic role labeling (also called shallow semantic parsing or slot-filling) is the process that assigns labels to words or phrases in a sentence that indicates their semantic role in the sentence, such as that of an agent, goal, or result. It serves to find the meaning of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc891ff",
   "metadata": {},
   "source": [
    "7.\tWhy Fine-tuning a BERT model takes less time than pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e1a397",
   "metadata": {},
   "source": [
    "Fine-tuning, conversely, follows the initial training, where a pre-trained model (previously trained on a vast dataset like ImageNet) is trained on a smaller, task-specific dataset. The rationale is to leverage the knowledge the model has acquired from the initial training process and tailor it to a more specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a0f9b",
   "metadata": {},
   "source": [
    "8.\tRecognizing Textual Entailment (RTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918422a",
   "metadata": {},
   "source": [
    "Textual Entailment Recognition has been proposed recently as a generic task that captures major semantic inference needs across many NLP applications, such as Question Answering, Information Retrieval, Information Extraction, and Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09876b4f",
   "metadata": {},
   "source": [
    "9.\tExplain the decoder stack of  GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bcd640",
   "metadata": {},
   "source": [
    "The decoder in a GPT model uses a specific type of attention mechanism known as masked self-attention. In a traditional transformer, the attention mechanism allows the model to focus on all parts of the input when generating each part of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247490fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
