{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef6d413b",
   "metadata": {},
   "source": [
    "1.\tExplain the basic architecture of RNN cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792c4f19",
   "metadata": {},
   "source": [
    "An RNN comprises a series of repeating neural network \"cells\" that are connected in a chain-like structure, where the output of one cell is passed as input to the next cell. Each cell inputs the current input to the network and the hidden state from the previous time step, producing an output and a new hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec2b56f",
   "metadata": {},
   "source": [
    "2.\tExplain Backpropagation through time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916f1e21",
   "metadata": {},
   "source": [
    "Backpropagation Through Time, or BPTT, is the application of the Backpropagation training algorithm to recurrent neural network applied to sequence data like a time series. A recurrent neural network is shown one input each timestep and predicts one output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d32a064",
   "metadata": {},
   "source": [
    "3.\tExplain Vanishing and exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be6f44e",
   "metadata": {},
   "source": [
    "Two opposite scenarios could happen in this case: the derivative term gets extremely small, i.e., approaches zero vs. this term gets extremely large and overflows. These issues are referred to as the Vanishing and Exploding Gradients, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7cbfbb",
   "metadata": {},
   "source": [
    "4.\tExplain Long short-term memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01032a7b",
   "metadata": {},
   "source": [
    "What is LSTM? LSTMs Long Short-Term Memory is a type of RNNs Recurrent Neural Network that can detain long-term dependencies in sequential data. LSTMs are able to process and analyze sequential data, such as time series, text, and speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1649539",
   "metadata": {},
   "source": [
    "5.\tExplain Gated recurrent unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9147c49",
   "metadata": {},
   "source": [
    "The Gated Recurrent Unit (GRU) is a type of Recurrent Neural Network (RNN) that, in certain cases, has advantages over long short term memory (LSTM). GRU uses less memory and is faster than LSTM, however, LSTM is more accurate when using datasets with longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b3d86b",
   "metadata": {},
   "source": [
    "6.\tExplain Peephole LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca478d9",
   "metadata": {},
   "source": [
    "Also, peephole connections allow gates to use the previous internal and hidden states (which the LSTM cell is limited to). This allows peephole LSTM to learn more precise timings than traditional LSTM cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce8191",
   "metadata": {},
   "source": [
    "7.\tBidirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b637893",
   "metadata": {},
   "source": [
    "Overview. Bi-directional recurrent neural networks (Bi-RNNs) are artificial neural networks that process input data in both the forward and backward directions. They are often used in natural language processing tasks, such as language translation, text classification, and named entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf0a4ff",
   "metadata": {},
   "source": [
    "8.\tExplain the gates of LSTM with equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e3ea08",
   "metadata": {},
   "source": [
    "It is an unit structure of LSTM, including 4 gates: input modulation gate, input gate, forget gate and output gate. We describe recurrent neural networks (RNNs), which have attracted great attention on sequential tasks, such as handwriting recognition, speech recognition and image to text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2a9663",
   "metadata": {},
   "source": [
    "9.\tExplain BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b8c9d",
   "metadata": {},
   "source": [
    "A bidirectional LSTM (BiLSTM) layer is an RNN layer that learns bidirectional long-term dependencies between time steps of time-series or sequence data. These dependencies can be useful when you want the RNN to learn from the complete time series at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e728879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
