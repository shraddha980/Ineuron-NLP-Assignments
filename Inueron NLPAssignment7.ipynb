{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c377522",
   "metadata": {},
   "source": [
    "1.\tWhat are Vanilla autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae6df6",
   "metadata": {},
   "source": [
    "The architecture of a vanilla autoencoder consists of an encoder followed by a decoder. The encoder maps the input to a hidden representation, typically through a series of fully-connected (dense) layers. The decoder maps the hidden representation back to the original input space through a series of dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8f9cb7",
   "metadata": {},
   "source": [
    "2.\tWhat are Sparse autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a40bad2",
   "metadata": {},
   "source": [
    "Sparse Autoencoders are a variant of autoencoders, which are neural networks trained to reconstruct their input data. However, unlike traditional autoencoders, sparse autoencoders are designed to be sensitive to specific types of high-level features in the data, while being insensitive to most other features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a4b074",
   "metadata": {},
   "source": [
    "3.\tWhat are Denoising autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefaa2cf",
   "metadata": {},
   "source": [
    "Denoising Autoencoders are neural network models that remove noise from corrupted or noisy data by learning to reconstruct the initial data from its noisy counterpart. We train the model to minimize the disparity between the original and reconstructed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19733cba",
   "metadata": {},
   "source": [
    "4.\tWhat are Convolutional autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374b0731",
   "metadata": {},
   "source": [
    "The convolutional Autoencoder is a type of neural network that can reduce noise in data by learning the underlying structure of the input. It comprises three parts: an Encoder that compresses the input while preserving useful features, a Bottleneck that selects important features, and a Decoder that reconstructs the data. The Encoder compresses the input data into a lower-dimensional representation, called the Bottleneck or latent space. The Decoder then takes the compressed input data and tries to reconstruct the original data. The bottleneck layer is responsible for choosing the most important features and passing them through to the Decoder. Training the Autoencoder involves comparing the original input data to the reconstructed data and adjusting the network weights to minimize the reconstruction error.\n",
    "\n",
    "Encoder Structure\n",
    "The Encoder part of the network is to compress the input data and passes it to the Bottleneck layer. The compression creates a knowledge representation much smaller than the original input but has most of its features.\n",
    "\n",
    "This part of the network comprises blocks of convolutions followed by pooling layers that, in turn, further help to create a compressed data representation. The output of an ideal Encoder should be the same as the input but with a smaller size. The Encoder should be sensitive to the inputs to recreate it and not over-sensitive. Being over-sensitive would make the model memorize the inputs perfectly and then overfit the data.\n",
    "\n",
    "Bottleneck Layer\n",
    "The Bottleneck is the most important layer of an Autoencoder. This module stores the compressed knowledge that is passed to the Decoder. The Bottleneck restricts information flow by only allowing important parts of the compressed representation to pass through to the Decoder. Doing so ensures that the input data has the maximum possible information extracted from it and the most useful correlations found. This part of the architecture is also a measure against overfitting as it prevents the network from memorizing the input data directly.\n",
    "\n",
    "Note that smaller bottlenecks lead to lesser overfitting (to an extent).\n",
    "\n",
    "Decoder Structure\n",
    "This part of the network is a \"Decompressor\" that attempts to recreate an image given its latent attributes. The Decoder gets the compressed information from the Bottleneck layer and then uses upsampling and convolutions to reconstruct it. The output generated by the Decoder is compared with the ground truth to quantify the network's performance.\n",
    "\n",
    "Latent Space Structure\n",
    "The latent space of a network is the compressed representation it creates from an input dataset. This latent space usually has hundreds of dimensions and is hard to visualize directly. More complex neural networks have latent spaces so hard to visualize that they are generally called black boxes. In a convolutional autoencoder, the better the representation of the data, the richer the latent space. The space structure here is a large matrix of tensors that encode the weights of network layers.\n",
    "\n",
    "Uses of Autoencoder\n",
    "The Convolutional Autoencoder architecture is good for a lot of use cases. Some of these are explained below.\n",
    "\n",
    "1. Reducing Complexity:\n",
    "\n",
    "The Encoder of the model works very well as a dimensionality reduction technique. For example, if we consider an image dataset, we can compress every image before feeding it to another model. This compression reduces the number of input values and thus makes the model less likely to be biased toward smaller details. The Autoencoder thus helps in improving the performance of a second model.\n",
    "\n",
    "2. Anomaly Detection:\n",
    "\n",
    "An Autoencoder is generally used for reconstructing the base data using an Encoder-Bottleneck-Decoder architecture. Thus if the output reconstruction has a much larger error for a given sample, this sample could be an outlier. We can thus use the reconstruction error to find unusual data points in a dataset.\n",
    "\n",
    "3. Unsupervised Learning:\n",
    "\n",
    "A convolutional Autoencoder's information compression ability is a good start for an unsupervised learning problem. Using the Autoencoder as a dimensionality reduction technique allows the data to be clustered without any labels much more easily. This clustering may not be useful but can be a starting point for many other solutions.\n",
    "\n",
    "4. Image Compression:\n",
    "\n",
    "Autoencoders can be used for image compression by training the network on a dataset of images and then using the encoder part of the network to compress new images. The compressed images can then be reconstructed by the decoder part of the network, resulting in a trade-off between image quality and compression ratio. This approach can achieve a high compression rate with minimal loss of image quality compared to traditional image compression methods. Autoencoders can also be used for lossy image compression, where some information is lost during the compression process, but the overall image quality is still maintained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93e914",
   "metadata": {},
   "source": [
    "5.\tWhat are Stacked autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2840581e",
   "metadata": {},
   "source": [
    "A Stacked Autoencoder is a neural network that is composed of multiple layers of autoencoders, where each layer is trained on the output of the previous one. This “stacking” of autoencoders allows the network to learn more complex representations of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f6bd9",
   "metadata": {},
   "source": [
    "6.\tExplain Extractive summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b63e22",
   "metadata": {},
   "source": [
    "Extractive summarization aims at identifying the salient information that is then extracted and grouped together to form a concise summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba75adc",
   "metadata": {},
   "source": [
    "7.\tExplain Abstractive summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f215f7",
   "metadata": {},
   "source": [
    "Abstractive summarization reformulates prominent concepts or ideas in the source text in clear language, often including words or sentences that do not exist in the source text. The hybrid method is a combination of these two approaches "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7e475",
   "metadata": {},
   "source": [
    "8.\tExplain Beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390e0e3e",
   "metadata": {},
   "source": [
    "In computer science, beam search is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set. Beam search is an optimization of best-first search that reduces its memory requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0c76db",
   "metadata": {},
   "source": [
    "9.\tExplain Length normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea190a7",
   "metadata": {},
   "source": [
    "Dividing the term frequency with the total number of words is called document length normalization. And, the reason why normalization is important is to minimize the effect of long vs. short documents and reflect the true importance of a keyword to a document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488aceea",
   "metadata": {},
   "source": [
    "10.\tExplain ROUGE metric evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b02b4e4",
   "metadata": {},
   "source": [
    "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It is essentially a set of metrics for evaluating automatic summarization of texts as well as machine translations. It works by comparing an automatically produced summary or translation against a set of reference summaries (typically human-produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18a1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
